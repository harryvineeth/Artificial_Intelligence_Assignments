
In the assignment different hyper parameters were tuned, the default optimizer when not mentioned is "Adam" and activation for the input and hidden layers is "Relu".

After all the experiments the best accuracy was obtained with Relu activation, Adam optimizer, batch size = '32'and two hidden layers.

Here the input is normalized between 0 to 1.
